{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[{}]\".format(8)) \\\n",
    "    .config(\"spark.driver.memory\", \"{}g\".format(16)) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "data_path = '/home/navid/data/rides_2018-09-01_2019-03-08.csv'\n",
    "\n",
    "ride_schema = StructType([\n",
    "    StructField(\"originLat\", FloatType(), True),\n",
    "    StructField(\"originLon\", FloatType(), True), \n",
    "    StructField(\"destinationLat\", FloatType(), True), \n",
    "    StructField(\"destinationLon\", FloatType(), True),\n",
    "    StructField(\"createdAt\", TimestampType(), True),\n",
    "    StructField(\"price\", FloatType(), True)])\n",
    "\n",
    "    \n",
    "ride_df_raw = spark.read.csv(data_path, schema=ride_schema, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[originLat: float, originLon: float, destinationLat: float, destinationLon: float, createdAt: timestamp, price: float]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter data\n",
    "\n",
    "start_date = '2018-09-01'\n",
    "end_date = '2018-11-01'\n",
    "\n",
    "lat_parts = 30\n",
    "lon_parts = 40\n",
    "\n",
    "min_lat, min_lon = 35.584246, 51.068630\n",
    "max_lat, max_lon = 35.809052, 51.646770\n",
    "\n",
    "ride_df_filtered = ride_df_raw.filter(ride_df_raw[\"createdAt\"] >= start_date).filter(ride_df_raw[\"createdAt\"] < end_date)\n",
    "ride_df_filtered = ride_df_filtered.filter(ride_df_filtered['originLat'] >= min_lat)\\\n",
    "                                   .filter(ride_df_filtered['originLon'] >= min_lon)\\\n",
    "                                   .filter(ride_df_filtered['originLat'] <= max_lat)\\\n",
    "                                   .filter(ride_df_filtered['originLon'] <= max_lon)\n",
    "    \n",
    "ride_df_filtered.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timeId: timestamp, rowId: int, colId: int]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def truncate(dt):\n",
    "    return dt - timedelta(\n",
    "        minutes=dt.minute % 5,\n",
    "        seconds=dt.second,\n",
    "        microseconds=dt.microsecond\n",
    "    )\n",
    "\n",
    "trunc_udf = F.udf(truncate, TimestampType())\n",
    "ride_df = ride_df_filtered.withColumn('timeId', trunc_udf(ride_df_filtered['createdAt']))\n",
    "ride_df = ride_df.withColumn('rowId', ((ride_df['originLat']-min_lat)/(max_lat-min_lat)*lat_parts).cast(IntegerType()))\n",
    "ride_df = ride_df.withColumn('colId', ((ride_df['originLon']-min_lon)/(max_lon-min_lon)*lon_parts).cast(IntegerType()))\n",
    "\n",
    "ride_df = ride_df.drop('originLat', 'originLon', 'createdAt', 'price', 'destinationLat', 'destinationLon')\n",
    "\n",
    "ride_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def generate_series(start, stop, interval):\n",
    "    \"\"\"\n",
    "    :param start  - lower bound, inclusive\n",
    "    :param stop   - upper bound, exclusive\n",
    "    :interval int - increment interval in seconds\n",
    "    \"\"\"\n",
    "    start, stop = spark.createDataFrame(\n",
    "        [(start, stop)], (\"start\", \"stop\")\n",
    "    ).select(\n",
    "        [col(c).cast(\"timestamp\").cast(\"long\") for c in (\"start\", \"stop\")\n",
    "    ]).first()\n",
    "\n",
    "    return spark.range(start, stop, interval).select(\n",
    "        col(\"id\").cast(\"timestamp\").alias(\"timeId\")\n",
    "    )\n",
    "\n",
    "complete_date_range = generate_series(start_date, end_date, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeIds: 17007, complete_timeIds: 17580\n"
     ]
    }
   ],
   "source": [
    "## NOTE THAT some time intervals may not have data\n",
    "\n",
    "timeIds = ride_df.select('timeId').distinct().collect()\n",
    "print('timeIds: {}, complete_timeIds: {}'.format(len(timeIds), complete_date_range.count()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rides_list = ride_df.sort('timeId').collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/4807757 data processed\n",
      "100000/4807757 data processed\n",
      "200000/4807757 data processed\n",
      "300000/4807757 data processed\n",
      "400000/4807757 data processed\n",
      "500000/4807757 data processed\n",
      "600000/4807757 data processed\n",
      "700000/4807757 data processed\n",
      "800000/4807757 data processed\n",
      "900000/4807757 data processed\n",
      "1000000/4807757 data processed\n",
      "1100000/4807757 data processed\n",
      "1200000/4807757 data processed\n",
      "1300000/4807757 data processed\n",
      "1400000/4807757 data processed\n",
      "1500000/4807757 data processed\n",
      "1600000/4807757 data processed\n",
      "1700000/4807757 data processed\n",
      "1800000/4807757 data processed\n",
      "1900000/4807757 data processed\n",
      "2000000/4807757 data processed\n",
      "2100000/4807757 data processed\n",
      "2200000/4807757 data processed\n",
      "2300000/4807757 data processed\n",
      "2400000/4807757 data processed\n",
      "2500000/4807757 data processed\n",
      "2600000/4807757 data processed\n",
      "2700000/4807757 data processed\n",
      "2800000/4807757 data processed\n",
      "2900000/4807757 data processed\n",
      "3000000/4807757 data processed\n",
      "3100000/4807757 data processed\n",
      "3200000/4807757 data processed\n",
      "3300000/4807757 data processed\n",
      "3400000/4807757 data processed\n",
      "3500000/4807757 data processed\n",
      "3600000/4807757 data processed\n",
      "3700000/4807757 data processed\n",
      "3800000/4807757 data processed\n",
      "3900000/4807757 data processed\n",
      "4000000/4807757 data processed\n",
      "4100000/4807757 data processed\n",
      "4200000/4807757 data processed\n",
      "4300000/4807757 data processed\n",
      "4400000/4807757 data processed\n",
      "4500000/4807757 data processed\n",
      "4600000/4807757 data processed\n",
      "4700000/4807757 data processed\n",
      "4800000/4807757 data processed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "last_time_id = None\n",
    "reqs = np.zeros((lat_parts, lon_parts))\n",
    "ride_windows = []\n",
    "\n",
    "for idx, ride in enumerate(rides_list):\n",
    "    \n",
    "    if last_time_id != ride.timeId:\n",
    "        if last_time_id is not None:\n",
    "            ride_windows.append(reqs)\n",
    "        reqs = np.zeros((lat_parts, lon_parts))\n",
    "        last_time_id = ride.timeId\n",
    "    \n",
    "    reqs[ride.rowId][ride.colId] += 1\n",
    "    \n",
    "    if idx % 100000 == 0:\n",
    "        print(\"{}/{} data processed\".format(idx, len(rides_list)))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of reult dataset: (17006, 30, 40)\n"
     ]
    }
   ],
   "source": [
    "ride_windows = np.array(ride_windows)\n",
    "\n",
    "print(\"shape of reult dataset: {}\".format(ride_windows.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./conv_lstm_sequence_data.pc', 'wb') as f:\n",
    "    pickle.dump(ride_windows, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

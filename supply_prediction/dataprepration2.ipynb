{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[{}]\".format(8)) \\\n",
    "    .config(\"spark.driver.memory\", \"{}g\".format(8)) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "data_path = '/home/navid/data/rides_2019-01-01_2019-07-01.csv'\n",
    "\n",
    "ride_schema = StructType([\n",
    "    StructField(\"createdAt\", TimestampType(), True),\n",
    "    StructField(\"latitude\", FloatType(), True),\n",
    "    StructField(\"longitude\", FloatType(), True)])\n",
    "\n",
    "    \n",
    "ride_df_raw = spark.read.csv(data_path, schema=ride_schema, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ride_df_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[createdAt: timestamp, latitude: float, longitude: float]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter data\n",
    "\n",
    "start_date = '2019-04-01'\n",
    "end_date = '2019-07-01'\n",
    "\n",
    "lat_parts = 20\n",
    "lon_parts = 30\n",
    "\n",
    "min_lat, min_lon = 35.64246, 51.168630\n",
    "max_lat, max_lon = 35.809052, 51.56770\n",
    "\n",
    "ride_df_filtered = ride_df_raw.filter(ride_df_raw[\"createdAt\"] >= start_date).filter(ride_df_raw[\"createdAt\"] < end_date)\n",
    "ride_df_filtered = ride_df_filtered.filter(ride_df_filtered['latitude'] >= min_lat)\\\n",
    "                                   .filter(ride_df_filtered['longitude'] >= min_lon)\\\n",
    "                                   .filter(ride_df_filtered['latitude'] <= max_lat)\\\n",
    "                                   .filter(ride_df_filtered['longitude'] <= max_lon)\n",
    "    \n",
    "ride_df_filtered.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ride_df_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[createdAt: timestamp, latitude: float, longitude: float, timeId: timestamp, rowId: int, colId: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def truncate(dt):\n",
    "    return dt - timedelta(\n",
    "        minutes=dt.minute % 5,\n",
    "        seconds=dt.second,\n",
    "        microseconds=dt.microsecond\n",
    "    )\n",
    "\n",
    "trunc_udf = F.udf(truncate, TimestampType())\n",
    "ride_df = ride_df_filtered.withColumn('timeId', trunc_udf(ride_df_filtered['createdAt']))\n",
    "ride_df = ride_df.withColumn('rowId', ((ride_df['latitude']-min_lat)/(max_lat-min_lat)*lat_parts).cast(IntegerType()))\n",
    "ride_df = ride_df.withColumn('colId', ((ride_df['longitude']-min_lon)/(max_lon-min_lon)*lon_parts).cast(IntegerType()))\n",
    "\n",
    "# ride_df = ride_df.drop('originLat', 'originLon', 'createdAt', 'price', 'destinationLat', 'destinationLon')\n",
    "\n",
    "ride_df.persist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def generate_series(start, stop, interval):\n",
    "    \"\"\"\n",
    "    :param start  - lower bound, inclusive\n",
    "    :param stop   - upper bound, exclusive\n",
    "    :interval int - increment interval in seconds\n",
    "    \"\"\"\n",
    "    start, stop = spark.createDataFrame(\n",
    "        [(start, stop)], (\"start\", \"stop\")\n",
    "    ).select(\n",
    "        [col(c).cast(\"timestamp\").cast(\"long\") for c in (\"start\", \"stop\")\n",
    "    ]).first()\n",
    "\n",
    "    return spark.range(start, stop, interval).select(\n",
    "        col(\"id\").cast(\"timestamp\").alias(\"timeId\")\n",
    "    )\n",
    "\n",
    "complete_date_range = generate_series(start_date, end_date, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ride_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntimeIds = ride_df.select('timeId').distinct().collect()\\nprint('timeIds: {}, complete_timeIds: {}'.format(len(timeIds), complete_date_range.count()))    \\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NOTE THAT some time intervals may not have data\n",
    "\"\"\"\n",
    "timeIds = ride_df.select('timeId').distinct().collect()\n",
    "print('timeIds: {}, complete_timeIds: {}'.format(len(timeIds), complete_date_range.count()))    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rides_list = ride_df.sort('timeId').collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/17489093 data processed\n",
      "100000/17489093 data processed\n",
      "200000/17489093 data processed\n",
      "300000/17489093 data processed\n",
      "400000/17489093 data processed\n",
      "500000/17489093 data processed\n",
      "600000/17489093 data processed\n",
      "700000/17489093 data processed\n",
      "800000/17489093 data processed\n",
      "900000/17489093 data processed\n",
      "1000000/17489093 data processed\n",
      "1100000/17489093 data processed\n",
      "1200000/17489093 data processed\n",
      "1300000/17489093 data processed\n",
      "1400000/17489093 data processed\n",
      "1500000/17489093 data processed\n",
      "1600000/17489093 data processed\n",
      "1700000/17489093 data processed\n",
      "1800000/17489093 data processed\n",
      "1900000/17489093 data processed\n",
      "2000000/17489093 data processed\n",
      "2100000/17489093 data processed\n",
      "2200000/17489093 data processed\n",
      "2300000/17489093 data processed\n",
      "2400000/17489093 data processed\n",
      "2500000/17489093 data processed\n",
      "2600000/17489093 data processed\n",
      "2700000/17489093 data processed\n",
      "2800000/17489093 data processed\n",
      "2900000/17489093 data processed\n",
      "3000000/17489093 data processed\n",
      "3100000/17489093 data processed\n",
      "3200000/17489093 data processed\n",
      "3300000/17489093 data processed\n",
      "3400000/17489093 data processed\n",
      "3500000/17489093 data processed\n",
      "3600000/17489093 data processed\n",
      "3700000/17489093 data processed\n",
      "3800000/17489093 data processed\n",
      "3900000/17489093 data processed\n",
      "4000000/17489093 data processed\n",
      "4100000/17489093 data processed\n",
      "4200000/17489093 data processed\n",
      "4300000/17489093 data processed\n",
      "4400000/17489093 data processed\n",
      "4500000/17489093 data processed\n",
      "4600000/17489093 data processed\n",
      "4700000/17489093 data processed\n",
      "4800000/17489093 data processed\n",
      "4900000/17489093 data processed\n",
      "5000000/17489093 data processed\n",
      "5100000/17489093 data processed\n",
      "5200000/17489093 data processed\n",
      "5300000/17489093 data processed\n",
      "5400000/17489093 data processed\n",
      "5500000/17489093 data processed\n",
      "5600000/17489093 data processed\n",
      "5700000/17489093 data processed\n",
      "5800000/17489093 data processed\n",
      "5900000/17489093 data processed\n",
      "6000000/17489093 data processed\n",
      "6100000/17489093 data processed\n",
      "6200000/17489093 data processed\n",
      "6300000/17489093 data processed\n",
      "6400000/17489093 data processed\n",
      "6500000/17489093 data processed\n",
      "6600000/17489093 data processed\n",
      "6700000/17489093 data processed\n",
      "6800000/17489093 data processed\n",
      "6900000/17489093 data processed\n",
      "7000000/17489093 data processed\n",
      "7100000/17489093 data processed\n",
      "7200000/17489093 data processed\n",
      "7300000/17489093 data processed\n",
      "7400000/17489093 data processed\n",
      "7500000/17489093 data processed\n",
      "7600000/17489093 data processed\n",
      "7700000/17489093 data processed\n",
      "7800000/17489093 data processed\n",
      "7900000/17489093 data processed\n",
      "8000000/17489093 data processed\n",
      "8100000/17489093 data processed\n",
      "8200000/17489093 data processed\n",
      "8300000/17489093 data processed\n",
      "8400000/17489093 data processed\n",
      "8500000/17489093 data processed\n",
      "8600000/17489093 data processed\n",
      "8700000/17489093 data processed\n",
      "8800000/17489093 data processed\n",
      "8900000/17489093 data processed\n",
      "9000000/17489093 data processed\n",
      "9100000/17489093 data processed\n",
      "9200000/17489093 data processed\n",
      "9300000/17489093 data processed\n",
      "9400000/17489093 data processed\n",
      "9500000/17489093 data processed\n",
      "9600000/17489093 data processed\n",
      "9700000/17489093 data processed\n",
      "9800000/17489093 data processed\n",
      "9900000/17489093 data processed\n",
      "10000000/17489093 data processed\n",
      "10100000/17489093 data processed\n",
      "10200000/17489093 data processed\n",
      "10300000/17489093 data processed\n",
      "10400000/17489093 data processed\n",
      "10500000/17489093 data processed\n",
      "10600000/17489093 data processed\n",
      "10700000/17489093 data processed\n",
      "10800000/17489093 data processed\n",
      "10900000/17489093 data processed\n",
      "11000000/17489093 data processed\n",
      "11100000/17489093 data processed\n",
      "11200000/17489093 data processed\n",
      "11300000/17489093 data processed\n",
      "11400000/17489093 data processed\n",
      "11500000/17489093 data processed\n",
      "11600000/17489093 data processed\n",
      "11700000/17489093 data processed\n",
      "11800000/17489093 data processed\n",
      "11900000/17489093 data processed\n",
      "12000000/17489093 data processed\n",
      "12100000/17489093 data processed\n",
      "12200000/17489093 data processed\n",
      "12300000/17489093 data processed\n",
      "12400000/17489093 data processed\n",
      "12500000/17489093 data processed\n",
      "12600000/17489093 data processed\n",
      "12700000/17489093 data processed\n",
      "12800000/17489093 data processed\n",
      "12900000/17489093 data processed\n",
      "13000000/17489093 data processed\n",
      "13100000/17489093 data processed\n",
      "13200000/17489093 data processed\n",
      "13300000/17489093 data processed\n",
      "13400000/17489093 data processed\n",
      "13500000/17489093 data processed\n",
      "13600000/17489093 data processed\n",
      "13700000/17489093 data processed\n",
      "13800000/17489093 data processed\n",
      "13900000/17489093 data processed\n",
      "14000000/17489093 data processed\n",
      "14100000/17489093 data processed\n",
      "14200000/17489093 data processed\n",
      "14300000/17489093 data processed\n",
      "14400000/17489093 data processed\n",
      "14500000/17489093 data processed\n",
      "14600000/17489093 data processed\n",
      "14700000/17489093 data processed\n",
      "14800000/17489093 data processed\n",
      "14900000/17489093 data processed\n",
      "15000000/17489093 data processed\n",
      "15100000/17489093 data processed\n",
      "15200000/17489093 data processed\n",
      "15300000/17489093 data processed\n",
      "15400000/17489093 data processed\n",
      "15500000/17489093 data processed\n",
      "15600000/17489093 data processed\n",
      "15700000/17489093 data processed\n",
      "15800000/17489093 data processed\n",
      "15900000/17489093 data processed\n",
      "16000000/17489093 data processed\n",
      "16100000/17489093 data processed\n",
      "16200000/17489093 data processed\n",
      "16300000/17489093 data processed\n",
      "16400000/17489093 data processed\n",
      "16500000/17489093 data processed\n",
      "16600000/17489093 data processed\n",
      "16700000/17489093 data processed\n",
      "16800000/17489093 data processed\n",
      "16900000/17489093 data processed\n",
      "17000000/17489093 data processed\n",
      "17100000/17489093 data processed\n",
      "17200000/17489093 data processed\n",
      "17300000/17489093 data processed\n",
      "17400000/17489093 data processed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "last_time_id = None\n",
    "reqs = np.zeros((lat_parts, lon_parts))\n",
    "ride_windows = []\n",
    "\n",
    "for idx, ride in enumerate(rides_list):\n",
    "    \n",
    "    if last_time_id != ride.timeId:\n",
    "        if last_time_id is not None:\n",
    "            ride_windows.append(reqs)\n",
    "        reqs = np.zeros((lat_parts, lon_parts))\n",
    "        last_time_id = ride.timeId\n",
    "    \n",
    "    reqs[ride.rowId][ride.colId] += 1\n",
    "    \n",
    "    if idx % 100000 == 0:\n",
    "        print(\"{}/{} data processed\".format(idx, len(rides_list)))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of reult dataset: (26129, 20, 30)\n"
     ]
    }
   ],
   "source": [
    "ride_windows = np.array(ride_windows)\n",
    "\n",
    "print(\"shape of reult dataset: {}\".format(ride_windows.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./conv_lstm_sequence_data6.pc', 'wb') as f:\n",
    "    pickle.dump(ride_windows, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run spark.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "start_date = '2018-05-01'\n",
    "end_date = '2018-06-01'\n",
    "\n",
    "ride_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"originId\", IntegerType(), True),\n",
    "    StructField(\"destinationId\", IntegerType(), True), \n",
    "    StructField(\"price\", FloatType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"path\", StringType(), True),\n",
    "    StructField(\"commission\", FloatType(), True),\n",
    "    StructField(\"isPassengerRated\", BooleanType(), True),\n",
    "    StructField(\"isDriverRated\", BooleanType(), True),\n",
    "    StructField(\"code\", StringType(), True), \n",
    "    StructField(\"passengerShare\", IntegerType(), True), \n",
    "    StructField(\"cancelStatus\", StringType(), True),\n",
    "    StructField(\"driverId\", IntegerType(), True), \n",
    "    StructField(\"passengerId\", IntegerType(), True),\n",
    "    StructField(\"congestionControl\", IntegerType(), True),\n",
    "    StructField(\"payment\", IntegerType(), True),\n",
    "    StructField(\"createdAt\", TimestampType(), True),\n",
    "    StructField(\"hasReturn\", BooleanType(), True),\n",
    "    StructField(\"returnPrice\", IntegerType(), True),\n",
    "    StructField(\"waitingTime\", IntegerType(), True),\n",
    "    StructField(\"waitingTimePrice\", IntegerType(), True),\n",
    "    StructField(\"dropAmount\", IntegerType(), True),\n",
    "    StructField(\"takerId\", IntegerType(), True),\n",
    "    StructField(\"creditInsufficientNotice\", IntegerType(), True),\n",
    "    StructField(\"rebatePercentage\", IntegerType(), True), \n",
    "    StructField(\"cancellationReasonCode\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"carCategoryId\", IntegerType(), True),\n",
    "    StructField(\"finishedAt\", TimestampType(), True),\n",
    "    StructField(\"estimatedETA\", DoubleType(), True),\n",
    "    StructField(\"googleDistance\", DoubleType(), True),\n",
    "    StructField(\"surgeCoefficient\", FloatType(), True),\n",
    "    StructField(\"driveId\", StringType(), True),\n",
    "    StructField(\"passengerCount\", IntegerType(), True),\n",
    "    StructField(\"initiatedVia\", StringType(), True), \n",
    "    StructField(\"driverEta\", IntegerType(), True),\n",
    "    StructField(\"passengerServiceCost\", IntegerType(), True),\n",
    "    StructField(\"matchCoefficient\", FloatType(), True),\n",
    "    StructField(\"carCategory\", StringType(), True), \n",
    "    StructField(\"discount\", IntegerType(), True),\n",
    "    StructField(\"uniqueId\", StringType(), True),\n",
    "    StructField(\"paymentMethod\", StringType(), True),\n",
    "    StructField(\"updatedAt\", TimestampType(), True)])\n",
    "\n",
    "location_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"longitude\", StringType(), True),\n",
    "    StructField(\"latitude\", StringType(), True), \n",
    "    StructField(\"address\", StringType(), True), \n",
    "    StructField(\"neighbourhoodCode\", StringType(), True), \n",
    "    StructField(\"congestionControl\", IntegerType(), True),\n",
    "    StructField(\"type\", StringType(), True), \n",
    "    StructField(\"createdAt\", TimestampType(), True), \n",
    "    StructField(\"updatedAt\", TimestampType(), True),\n",
    "    StructField(\"rideId\", IntegerType(), True),\n",
    "    StructField(\"seqNum\", IntegerType(), True),\n",
    "    StructField(\"shortAddress\", StringType(), True)])  \n",
    "    \n",
    "    \n",
    "ride_df_raw = spark.read.csv('./ride_2018_05.csv', schema=ride_schema, header=False)\n",
    "location_df_raw = spark.read.csv('./location_2018_05.csv', schema=location_schema, header=False) \n",
    "\n",
    "ride_df_raw = ride_df_raw.filter(ride_df_raw[\"createdAt\"] >= start_date).filter(ride_df_raw[\"createdAt\"] < end_date)\n",
    "location_df = location_df_raw.filter(location_df_raw[\"createdAt\"] >= start_date).filter(location_df_raw[\"createdAt\"] < end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ride_df = ride_df_raw.sample(fraction=1.0)\n",
    "\n",
    "ride_df = sample_ride_df\n",
    "print(\"len ride_df={}\".format(ride_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# filter unique requests\n",
    "INF = 1000000000\n",
    "VALID_CREATED_AT_DIFF = 1200\n",
    "\n",
    "# filter city\n",
    "ride_df = ride_df.filter(ride_df[\"city\"] == 'TEHRAN')\n",
    "\n",
    "\n",
    "# todo: we did not use latlongcode\n",
    "passenger_window = Window.partitionBy(\"passengerId\").orderBy(\"createdAt\")\n",
    "ride_df = ride_df.withColumn(\"prevCreatedAt\", F.lag(ride_df[\"createdAt\"]).over(passenger_window))\n",
    "ride_df = ride_df.withColumn(\"diffCreatedAt\", F.when(F.isnull(ride_df[\"prevCreatedAt\"]), INF).otherwise(F.unix_timestamp(ride_df[\"createdAt\"]) - F.unix_timestamp(ride_df[\"prevCreatedAt\"])))\n",
    "ride_df = ride_df.filter(ride_df[\"diffCreatedAt\"] > VALID_CREATED_AT_DIFF)\n",
    "\n",
    "\n",
    "# filter short cancellations\n",
    "# TODO: why don't we remove all cancelled rides\n",
    "MINIMUM_VALID_CANCELLATION_DELAY = 10\n",
    "\n",
    "ride_df = ride_df.filter((ride_df[\"status\"] != 'CANCELED') | (F.unix_timestamp(ride_df[\"updatedAt\"]) - F.unix_timestamp(ride_df[\"createdAt\"]) > MINIMUM_VALID_CANCELLATION_DELAY))\n",
    "\n",
    "\n",
    "# filter invalid distances\n",
    "MINIMUM_VALID_DISTANCE = 0\n",
    "MAXIMUM_VALID_DISTANCE = 65000\n",
    "\n",
    "ride_df = ride_df.filter(ride_df[\"googleDistance\"] > MINIMUM_VALID_DISTANCE).filter(ride_df[\"googleDistance\"] < MAXIMUM_VALID_DISTANCE)\n",
    "\n",
    "\n",
    "# filter invalid etas\n",
    "MINIMUM_VALID_ETA = 180\n",
    "MAXIMUM_VALID_ETA = 10800\n",
    "\n",
    "ride_df = ride_df.filter(ride_df[\"estimatedETA\"] > MINIMUM_VALID_ETA).filter(ride_df[\"estimatedETA\"] < MAXIMUM_VALID_ETA)\n",
    "\n",
    "# filter invalid speeds\n",
    "# todo: definition of speed\n",
    "MAXIMUM_VALID_SPEED = 28\n",
    "\n",
    "ride_df = ride_df.withColumn(\"speed\", ride_df[\"googleDistance\"]/ride_df[\"estimatedETA\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[datetimeId: timestamp, gridId: string, count: bigint, surge: double]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# find region of rides\n",
    "def get_grid_id(latitude, longitude):\n",
    "    return \"{}_{}\".format(\n",
    "        int(latitude/0.016486436999999996),\n",
    "        int(longitude/0.030950938)\n",
    "    )\n",
    "udf_get_grid_id = F.udf(get_grid_id, StringType())\n",
    "\n",
    "surge_df = ride_df\n",
    "\n",
    "surge_df = surge_df.join(location_df, surge_df[\"originId\"] == location_df[\"id\"]).drop(location_df[\"createdAt\"]).drop(location_df[\"id\"])\n",
    "surge_df = surge_df.withColumn(\"originLatitude\", surge_df[ \"latitude\"].cast(DoubleType())).drop(surge_df[\"latitude\"])\n",
    "surge_df = surge_df.withColumn(\"originLongitude\", surge_df[\"longitude\"].cast(DoubleType())).drop(surge_df[\"longitude\"])\n",
    "\n",
    "surge_df = surge_df.withColumn(\"gridId\", udf_get_grid_id(surge_df[\"originLatitude\"], surge_df[\"originLongitude\"]))\n",
    "\n",
    "# truncate the datetime\n",
    "def get_datetime_id(dt):\n",
    "    td = timedelta(\n",
    "        minutes=dt.minute % 5,\n",
    "        seconds=dt.second,\n",
    "        microseconds=dt.microsecond)\n",
    "\n",
    "    return dt - td\n",
    "udf_get_datetime_id = F.udf(get_datetime_id, TimestampType())\n",
    "\n",
    "surge_df = surge_df.withColumn(\"datetimeId\", udf_get_datetime_id(surge_df[\"createdAt\"]))\n",
    "\n",
    "speed_df = surge_df.groupBy('datetimeId').agg(F.avg('speed').alias('speed'))\n",
    "region_df = surge_df.groupBy(surge_df[\"datetimeId\"], surge_df[\"gridId\"]).agg(F.count(surge_df[\"id\"]).alias(\"count\"), F.avg(surge_df[\"surgeCoefficient\"]).alias(\"surge\"))\n",
    "\n",
    "speed_df.cache()\n",
    "region_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634352\n",
      "+-------------------+---------+-----+------------------+\n",
      "|         datetimeId|   gridId|count|             surge|\n",
      "+-------------------+---------+-----+------------------+\n",
      "|2018-05-01 06:20:00|2169_1665|    3|               1.0|\n",
      "|2018-05-01 08:20:00|2164_1658|    9|               1.0|\n",
      "|2018-05-02 11:10:00|2169_1663|    2| 0.949999988079071|\n",
      "|2018-05-02 15:50:00|2167_1660|    6| 0.800000011920929|\n",
      "|2018-05-02 23:15:00|2170_1662|    5|1.2000000476837158|\n",
      "|2018-05-05 06:00:00|2170_1662|    2| 1.100000023841858|\n",
      "|2018-05-06 11:05:00|2166_1662|    4| 0.949999988079071|\n",
      "|2018-05-06 18:55:00|2168_1659|    3| 0.800000011920929|\n",
      "|2018-05-06 19:25:00|2170_1662|    6|0.8666666547457377|\n",
      "|2018-05-07 20:30:00|2166_1662|    7|1.3285714047295707|\n",
      "|2018-05-08 21:05:00|2166_1664|    2|0.8999999761581421|\n",
      "|2018-05-09 08:30:00|2169_1659|   18|1.0666666825612385|\n",
      "|2018-05-09 09:35:00|2172_1664|    2|               1.0|\n",
      "|2018-05-09 10:50:00|2167_1664|   10| 0.949999988079071|\n",
      "|2018-05-10 12:35:00|2167_1661|   24|0.8416666636864344|\n",
      "|2018-05-11 16:15:00|2169_1661|    7| 1.085714306150164|\n",
      "|2018-05-12 12:00:00|2165_1660|   13| 0.800000011920929|\n",
      "|2018-05-12 21:05:00|2168_1659|    8|0.8874999806284904|\n",
      "|2018-05-13 09:55:00|2163_1661|    4|               1.0|\n",
      "|2018-05-13 21:20:00|2169_1660|    4| 1.100000038743019|\n",
      "+-------------------+---------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(region_df.count())\n",
    "region_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-----+-----+\n",
      "|   gridId|         datetimeId|count|surge|\n",
      "+---------+-------------------+-----+-----+\n",
      "|2147_1652|2018-05-01 00:00:00|    0|  1.0|\n",
      "|2147_1652|2018-05-01 00:05:00|    0|  1.0|\n",
      "|2147_1652|2018-05-01 00:10:00|    1|  1.0|\n",
      "|2147_1652|2018-05-01 00:15:00|    0|  1.0|\n",
      "|2147_1652|2018-05-01 00:20:00|    1|  1.0|\n",
      "|2147_1652|2018-05-01 00:25:00|    0|  1.0|\n",
      "|2147_1652|2018-05-01 00:30:00|    2|  1.0|\n",
      "|2147_1652|2018-05-01 00:35:00|    1|  1.0|\n",
      "|2147_1652|2018-05-01 00:40:00|    1|  1.0|\n",
      "|2147_1652|2018-05-01 00:45:00|    0|  1.0|\n",
      "|2147_1652|2018-05-01 00:50:00|    0|  1.0|\n",
      "|2147_1652|2018-05-01 00:55:00|    1|  1.0|\n",
      "|2147_1652|2018-05-01 01:00:00|    0|  1.0|\n",
      "|2147_1652|2018-05-01 01:05:00|    0|  1.0|\n",
      "|2147_1652|2018-05-01 01:10:00|    0|  1.0|\n",
      "|2147_1652|2018-05-01 01:15:00|    0|  1.0|\n",
      "|2147_1652|2018-05-01 01:20:00|    0|  1.0|\n",
      "|2147_1652|2018-05-01 01:25:00|    1|  1.0|\n",
      "|2147_1652|2018-05-01 01:30:00|    0|  1.0|\n",
      "|2147_1652|2018-05-01 01:35:00|    1|  1.0|\n",
      "+---------+-------------------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "#region_df = surge_df.groupBy(surge_df[\"datetimeId\"], surge_df[\"gridId\"]).agg(F.count(surge_df[\"id\"]).alias(\"count\"), F.avg(surge_df[\"surgeCoefficient\"]).alias(\"surge\"))\n",
    "#region_df = region_df.withColumn(\"date\", region_df[\"datetimeId\"].cast(DateType()))\n",
    "#top_region_df = region_df.groupBy(region_df[\"gridId\"], region_df[\"date\"]).agg(F.sum(region_df[\"count\"]).alias(\"sum\"))\n",
    "\n",
    "top_region_df = region_df.withColumn(\"date\", F.to_date('datetimeId')).groupBy(\"gridId\", \"date\").agg(F.sum(\"count\").alias(\"sum\"))\n",
    "top_region_df = top_region_df.groupBy(\"gridId\").agg(F.mean(\"sum\").alias(\"mean\"))\n",
    "top_region_df = top_region_df.filter(F.col(\"mean\") > 20)\n",
    "\n",
    "region_filtered_df = region_df.join(top_region_df, 'gridId').drop('mean')\n",
    "\n",
    "datetime_id_complete_df = region_filtered_df.select(\"datetimeId\").distinct()\n",
    "region_complete_df = datetime_id_complete_df.crossJoin(top_region_df.select('gridId'))\n",
    "\n",
    "#region_complete_df.printSchema()\n",
    "#region_df_filtered.printSchema()\n",
    "\n",
    "#region_complete_df.show()\n",
    "#region_df_filtered.show()\n",
    "\n",
    "#region_complete_df = region_complete_df.join(region_df_filtered, (region_df_filtered[\"gridId\"] == region_complete_df[\"gridId\"]), how='left')\n",
    "region_complete_df = region_complete_df.join(region_filtered_df.toDF(*region_filtered_df.columns),\n",
    "                    [\"gridId\", \"datetimeId\"], how='left').fillna({'count': 0, 'surge': 1.0}).persist()\n",
    "#region_complete_df.count()\n",
    "\n",
    "#region_complete_df.printSchema()\n",
    "\n",
    "\n",
    "region_complete_df.sort(['gridId', 'datetimeId']).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534302"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_complete_df.filter(region_complete_df['count'] == 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_df = region_complete_df.withColumn('date', F.to_date('datetimeId'))\n",
    "feature_df = feature_df.withColumn('hour', F.hour('datetimeId'))\n",
    "hour_translator = ['0-5'] * 6 + ['6-8'] * 3 + ['9-15'] * 7 + ['16-18'] * 3 + ['19-23'] * 5\n",
    "feature_df = feature_df.withColumn('hourGroup', F.udf((lambda h: hour_translator[h]), StringType())('hour'))\n",
    "feature_df = feature_df.withColumn('weekday', F.dayofweek('datetimeId'))\n",
    "feature_df = feature_df.withColumn('yearday', F.dayofyear('datetimeId'))\n",
    "feature_df = feature_df.join(speed_df, 'datetimeId', how='left').fillna({'speed': 28}) # todo: max speed\n",
    "feature_df = feature_df.withColumn(\"speedLaged\", F.lag(\"speed\").over(Window.partitionBy(\"gridId\").orderBy(\"datetimeId\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-----+-----+----------+----+---------+-------+-------+------------------+------------------+-------------------+-------------------+\n",
      "|         datetimeId|   gridId|count|surge|      date|hour|hourGroup|weekday|yearday|             speed|        speedLaged|          hamsCount|     hamsCountLaged|\n",
      "+-------------------+---------+-----+-----+----------+----+---------+-------+-------+------------------+------------------+-------------------+-------------------+\n",
      "|2018-05-01 00:00:00|2161_1660|    2|  1.0|2018-05-01|   0|      0-5|      3|    121| 9.923103428743413|              null|0.14285714285714285|               null|\n",
      "|2018-05-01 00:05:00|2161_1660|    0|  1.0|2018-05-01|   0|      0-5|      3|    121| 9.681033710711315| 9.923103428743413| 0.2857142857142857|0.14285714285714285|\n",
      "|2018-05-01 00:10:00|2161_1660|    1|  1.0|2018-05-01|   0|      0-5|      3|    121|10.218804663134165| 9.681033710711315|0.14285714285714285| 0.2857142857142857|\n",
      "|2018-05-01 00:15:00|2161_1660|    1|  1.0|2018-05-01|   0|      0-5|      3|    121|10.406467919413833|10.218804663134165| 0.2857142857142857|0.14285714285714285|\n",
      "|2018-05-01 00:20:00|2161_1660|    0|  1.0|2018-05-01|   0|      0-5|      3|    121|10.368483789869336|10.406467919413833|0.42857142857142855| 0.2857142857142857|\n",
      "|2018-05-01 00:25:00|2161_1660|    3|  1.0|2018-05-01|   0|      0-5|      3|    121|10.546432912827537|10.368483789869336| 0.2857142857142857|0.42857142857142855|\n",
      "|2018-05-01 00:30:00|2161_1660|    1|  1.0|2018-05-01|   0|      0-5|      3|    121|10.532820565846631|10.546432912827537| 0.2857142857142857| 0.2857142857142857|\n",
      "|2018-05-01 00:35:00|2161_1660|    0|  1.0|2018-05-01|   0|      0-5|      3|    121|10.062769853737748|10.532820565846631|0.42857142857142855| 0.2857142857142857|\n",
      "|2018-05-01 00:40:00|2161_1660|    0|  1.0|2018-05-01|   0|      0-5|      3|    121| 10.58422442012573|10.062769853737748|0.14285714285714285|0.42857142857142855|\n",
      "|2018-05-01 00:45:00|2161_1660|    0|  1.0|2018-05-01|   0|      0-5|      3|    121|10.477150942746922| 10.58422442012573|0.42857142857142855|0.14285714285714285|\n",
      "|2018-05-01 00:50:00|2161_1660|    0|  1.0|2018-05-01|   0|      0-5|      3|    121|10.613259984511567|10.477150942746922|0.14285714285714285|0.42857142857142855|\n",
      "|2018-05-01 00:55:00|2161_1660|    1|  1.0|2018-05-01|   0|      0-5|      3|    121|10.334336743070907|10.613259984511567|                0.0|0.14285714285714285|\n",
      "|2018-05-01 01:00:00|2161_1660|    1|  1.0|2018-05-01|   1|      0-5|      3|    121|10.266500916419409|10.334336743070907|0.14285714285714285|                0.0|\n",
      "|2018-05-01 01:05:00|2161_1660|    0|  1.0|2018-05-01|   1|      0-5|      3|    121|10.386711153770811|10.266500916419409|                0.0|0.14285714285714285|\n",
      "|2018-05-01 01:10:00|2161_1660|    1|  1.0|2018-05-01|   1|      0-5|      3|    121|10.250334000587898|10.386711153770811|                0.0|                0.0|\n",
      "|2018-05-01 01:15:00|2161_1660|    0|  1.0|2018-05-01|   1|      0-5|      3|    121|10.399512615537278|10.250334000587898|0.14285714285714285|                0.0|\n",
      "|2018-05-01 01:20:00|2161_1660|    2|  1.0|2018-05-01|   1|      0-5|      3|    121|10.680023647475386|10.399512615537278|0.14285714285714285|0.14285714285714285|\n",
      "|2018-05-01 01:25:00|2161_1660|    0|  1.0|2018-05-01|   1|      0-5|      3|    121|11.715374178717981|10.680023647475386| 0.2857142857142857|0.14285714285714285|\n",
      "|2018-05-01 01:30:00|2161_1660|    0|  1.0|2018-05-01|   1|      0-5|      3|    121|10.419143572550656|11.715374178717981| 0.2857142857142857| 0.2857142857142857|\n",
      "|2018-05-01 01:35:00|2161_1660|    0|  1.0|2018-05-01|   1|      0-5|      3|    121|10.538560967810191|10.419143572550656|0.14285714285714285| 0.2857142857142857|\n",
      "+-------------------+---------+-----+-----+----------+----+---------+-------+-------+------------------+------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def f(grid_id):\n",
    "    x, y = [int(x) for x in grid_id.split('_')]\n",
    "    return [('{}_{}'.format(x+i,y+j)) for i in [-1,0,1] for j in [-1,0,1] if i!=0 or j!=0]\n",
    "\n",
    "hams_func = F.udf(f, ArrayType(StringType()))\n",
    "\n",
    "hams = (feature_df.select('datetimeId', 'count', F.explode(hams_func('gridId')).alias('gridId')).\n",
    "        groupby('datetimeId', 'gridId').agg(F.mean('count').alias('hamsCount')))\n",
    "\n",
    "feature_df = feature_df.join(hams.toDF(*hams.columns), ['datetimeId', 'gridId'], how='left').\\\n",
    "    fillna({'hamsCount': 0}).withColumn(\"hamsCountLaged\", F.lag(\"hamsCount\").\\\n",
    "    over(Window.partitionBy(\"gridId\").orderBy(\"datetimeId\")))\n",
    "\n",
    "feature_df.cache()\n",
    "feature_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "window = Window.partitionBy(\"gridId\").orderBy(\"datetimeId\")\n",
    "feature_df = (feature_df\n",
    "              .withColumn('m1', F.lag('count', count=1).over(window))\n",
    "              .withColumn('m2', F.lag('count', count=2).over(window))\n",
    "              .withColumn('m3', F.lag('count', count=3).over(window))\n",
    "              .withColumn('d0', F.lag('count', count=24*12-1).over(window))\n",
    "              .withColumn('d1', F.lag('count', count=24*12).over(window))\n",
    "              .withColumn('d2', F.lag('count', count=24*12+1).over(window))\n",
    "              .withColumn('w0', F.lag('count', count=7*24*12-1).over(window))\n",
    "              .withColumn('w1', F.lag('count', count=7*24*12).over(window))\n",
    "              .withColumn('w2', F.lag('count', count=7*24*12+1).over(window))\n",
    "              )\n",
    "\n",
    "feature_df = (feature_df\n",
    "              .withColumn('unixts', F.unix_timestamp('datetimeId'))\n",
    "              .withColumn('sd', F.sin(2*math.pi*F.col('unixts')/(24*3600)))\n",
    "              .withColumn('cd', F.cos(2*math.pi*F.col('unixts')/(24*3600)))\n",
    "              .withColumn('sw', F.sin(2*math.pi*F.col('unixts')/(7*24*3600)))\n",
    "              .withColumn('cw', F.cos(2*math.pi*F.col('unixts')/(7*24*3600)))\n",
    "              .drop('unixts')\n",
    "              )\n",
    "\n",
    "#TODO: add poisson regressor\n",
    "\n",
    "feature_df.write.csv('./feature.csv', mode='overwrite', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.coalesce(1).write.csv('./feature.csv', mode='overwrite', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
